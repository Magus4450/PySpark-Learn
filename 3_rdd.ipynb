{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resilient Distributed Dataset\n",
    "\n",
    "\n",
    "* Each record in RDD in divided into logical partitions, which can be computed on different nodes of the cluster\n",
    "\n",
    "* RDD is computed on several processes scattered across multiple physical servers called nodes\n",
    "\n",
    "## Advantages\n",
    "\n",
    "* In-Memory Processing\n",
    "    * loads data from disk and process in memory and keeps the data in memory\n",
    "    * can cache RDD in memory to reuse\n",
    "\n",
    "* Immutability\n",
    "    \n",
    "* Fault Tolerance\n",
    "\n",
    "* Lazy Evaluation\n",
    "\n",
    "* Partitioning\n",
    "\n",
    "\n",
    "## Limitations\n",
    "\n",
    "not suitable for applications that make updates to the state store such as storage system for a web app\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD are mainly created in two ways\n",
    "\n",
    "# Parallelizing an exising collection\n",
    "# Referencing dataset in external storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bash: /home/magus/miniconda3/envs/nlp/lib/libtinfo.so.6: no version information available (required by bash)\n",
      "bash: /home/magus/miniconda3/envs/nlp/lib/libtinfo.so.6: no version information available (required by bash)\n",
      "22/12/21 19:32:58 WARN Utils: Your hostname, Magus resolves to a loopback address: 127.0.1.1; using 172.26.192.58 instead (on interface eth0)\n",
      "22/12/21 19:32:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/magus/spark-3.1.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/12/21 19:32:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/12/21 19:33:00 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.26.192.58:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[1]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>RDD</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f85dae40310>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "        .appName(\"RDD\") \\\n",
    "        .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parallelizing\n",
    "\n",
    "data = list(range(1,13))\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "endomondoHR.json MapPartitionsRDD[2] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = spark.sparkContext.textFile(\"endomondoHR.json\")\n",
    "rdd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmptyRDD[3] at emptyRDD at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating empty RDD\n",
    "\n",
    "rdd3 = spark.sparkContext.emptyRDD()\n",
    "rdd3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[4] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Empty RDD with partitions\n",
    "\n",
    "rdd4 = spark.sparkContext.parallelize([], 10) # 10 partitions\n",
    "rdd4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallelize automatically creates partitions based on resource availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Repartition and Coalesce\n",
    "\n",
    "# Repartition -> Shuffles data from all nodes \n",
    "# Coalesce -> Shuffle data from minimum nodes\n",
    "\n",
    "reparRdd = rdd.repartition(4)\n",
    "reparRdd.getNumPartitions()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD Transformation are lazy meaning they return another\n",
    "# RDD instead of updating the current one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.read.json(\"./endomondoHR.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.textFile(\"./test.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[12] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flatmap flattens the RDD after applying the function and returns a new RDD\n",
    "\n",
    "rdd2 = rdd.flatMap(lambda x: x.split(\" \"))\n",
    "rdd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Project Gutenberg’s',\n",
       " 'Alice’s Adventures in Wonderland',\n",
       " 'by Lewis Carroll',\n",
       " 'This eBook is for the use',\n",
       " 'of anyone anywhere',\n",
       " 'at no cost and with',\n",
       " 'Alice’s Adventures in Wonderland',\n",
       " 'by Lewis Carroll',\n",
       " 'This eBook is for the use',\n",
       " 'of anyone anywhere']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View data of rdd\n",
    "rdd.collect()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Project',\n",
       " 'Gutenberg’s',\n",
       " 'Alice’s',\n",
       " 'Adventures',\n",
       " 'in',\n",
       " 'Wonderland',\n",
       " 'by',\n",
       " 'Lewis',\n",
       " 'Carroll',\n",
       " 'This']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.collect()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Project', 'Gutenberg’s'],\n",
       " ['Alice’s', 'Adventures', 'in', 'Wonderland'],\n",
       " ['by', 'Lewis', 'Carroll'],\n",
       " ['This', 'eBook', 'is', 'for', 'the', 'use'],\n",
       " ['of', 'anyone', 'anywhere'],\n",
       " ['at', 'no', 'cost', 'and', 'with'],\n",
       " ['Alice’s', 'Adventures', 'in', 'Wonderland'],\n",
       " ['by', 'Lewis', 'Carroll'],\n",
       " ['This', 'eBook', 'is', 'for', 'the', 'use'],\n",
       " ['of', 'anyone', 'anywhere']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3 = rdd.map(lambda x: x.split(\" \"))\n",
    "rdd3.collect()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Project', 1),\n",
       " ('Gutenberg’s', 1),\n",
       " ('Alice’s', 1),\n",
       " ('Adventures', 1),\n",
       " ('in', 1),\n",
       " ('Wonderland', 1),\n",
       " ('by', 1),\n",
       " ('Lewis', 1),\n",
       " ('Carroll', 1),\n",
       " ('This', 1)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd4 = rdd2.map(lambda x: (x,1))\n",
    "rdd4.collect()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Project', 9),\n",
       " ('Gutenberg’s', 9),\n",
       " ('Alice’s', 18),\n",
       " ('Adventures', 18),\n",
       " ('in', 18),\n",
       " ('Wonderland', 18),\n",
       " ('by', 18),\n",
       " ('Lewis', 18),\n",
       " ('Carroll', 18),\n",
       " ('This', 27)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ReduceByKey merges value for each key with the function provided\n",
    "\n",
    "rdd5 = rdd4.reduceByKey(lambda x,y: x+y)\n",
    "rdd5.collect()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(27, 'This'),\n",
       " (27, 'eBook'),\n",
       " (27, 'is'),\n",
       " (27, 'for'),\n",
       " (27, 'the'),\n",
       " (27, 'use'),\n",
       " (27, 'of'),\n",
       " (27, 'anyone'),\n",
       " (27, 'anywhere'),\n",
       " (27, 'at')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SortByKey is used to sort RDD elements on key\n",
    "\n",
    "# For PairRDD above of (string, int)\n",
    "# we first convert it into (int, string)\n",
    "# Then we used sortByKey which indeally sorts on int value\n",
    "rdd6 = rdd5.map(lambda x: (x[1], x[0])).sortByKey(ascending=False)\n",
    "rdd6.collect()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Wonderland', 1),\n",
       " ('anyone', 1),\n",
       " ('anywhere', 1),\n",
       " ('and', 1),\n",
       " ('Wonderland', 1),\n",
       " ('anyone', 1),\n",
       " ('anywhere', 1),\n",
       " ('and', 1),\n",
       " ('anyone', 1),\n",
       " ('anywhere', 1)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter is used to filter records in RDD\n",
    "\n",
    "rdd5 = rdd4.filter(lambda x: 'an' in x[0])\n",
    "rdd5.collect()[:10]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Actions\n",
    "\n",
    "RDD Action operations return values from an RDD. RDD functions that returns non-RDD is considered as action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count\n",
    "\n",
    "rdd6.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 'This')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First\n",
    "\n",
    "rdd6.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 'with')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Max\n",
    "\n",
    "rdd6.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(522, '')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reduce reduces records to single, can use to count or sum\n",
    "\n",
    "totalWordCount = rdd6.reduce(lambda a,b : (a[0]+b[0],''))\n",
    "totalWordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(27, 'This'), (27, 'eBook'), (27, 'is')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take returns record specified as an argument\n",
    "\n",
    "data3 = rdd6.take(3)\n",
    "data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect returns all data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle Operations\n",
    "\n",
    "Used to redistribute data across different executors or across machines\n",
    "\n",
    "Expensive task since it involves\n",
    "\n",
    "* Dist I/O\n",
    "* Involves data serialization and deserialization\n",
    "* Network I/O"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens when we first run reduceByKey\n",
    "\n",
    "Initially when we create an RDD, partitions do not store data for all keys.\n",
    "\n",
    "* PySpark runs map tasks on all partitions which groups all values for a single key\n",
    "* Reulsts of the map tasks are kept in memory\n",
    "* When results do not fit in mem, PySpark sotres data into disk\n",
    "* PS shuffles the mapped data across partitions, some times it also stores the shuffles data into a dist for reuse \n",
    "* Run the garbage collection\n",
    "* Finally runs reduce taks on each partition based on key"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Persistence\n",
    "\n",
    "Even though PS is 100x times faster than traditional MapReduce jobs, if we do not design jobs to reuse repeating computations, we will see performance drop.\n",
    "\n",
    "Using ```cache()``` and ```persists()``` methods, PS provides optimization mechanism to store the intermediate computation.\n",
    "\n",
    "### Advantages\n",
    "\n",
    "* Cost Efficient\n",
    "* Time Efficient\n",
    "* Execution Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "./test.txt MapPartitionsRDD[11] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cache by default saved RDD computation to storage level 'MEMEORY_ONLY' meaning it will store data in the JVM heap as unserialized objects\n",
    "\n",
    "# cache() internally calls persists()\n",
    "\n",
    "cachedRdd = rdd.cache()\n",
    "cachedRdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "./test.txt MapPartitionsRDD[11] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# persist() is used to store RDD to one of many storage levels\n",
    "\n",
    "import pyspark\n",
    "\n",
    "dfPersist = rdd.persist(pyspark.StorageLevel.MEMORY_ONLY)\n",
    "dfPersist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "./test.txt MapPartitionsRDD[11] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PS automatically removes persist and cache if not used\n",
    "# Can manually remove by unpersist()\n",
    "\n",
    "rddPersist2 = dfPersist.unpersist()\n",
    "rddPersist2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark Shared Variables\n",
    "\n",
    "When PS does transformations using ```map()``` or ```reduce()```, it executes tranformations on remote node by using variables that are shipped with the taks and these variables are not sent back to PS Driver. Hence, we cannot share variables across tasks.\n",
    "\n",
    "PS provides shared variables to solve this.\n",
    "* Broadcast Variables (read-only)\n",
    "* Accumulator Variables (updatable)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcast\n",
    "\n",
    "* read-only\n",
    "* cached and available on all nodes in a cluster\n",
    "* instead of sending this data along with every task,PS distributes broadcast variables to the machines using efficient broadcast algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcastVar = spark.sparkContext.broadcast([0,1,2,3])\n",
    "broadcastVar.value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accumulators\n",
    "\n",
    "* 'added' through an associative and commutative operations\n",
    "* used to perform counters\n",
    "\n",
    "Types\n",
    "* named: shown in PS web UI\n",
    "* unnamed: not shown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accum = spark.sparkContext.accumulator(0.0)\n",
    "spark.sparkContext.parallelize([1,2,3,4]).foreach(lambda x: accum.add(x))\n",
    "accum.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "df63add32a7940b2e1c5aa462fbf455c5b69f07faba4f5e92570f0b1dca1de6b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
